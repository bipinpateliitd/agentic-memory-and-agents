{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot from Scratch with LangChain\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "In this beginner-friendly tutorial, you'll learn to build a chatbot step by step:\n",
    "\n",
    "1. **Basic Chatbot (No Memory)** - Start with a simple chatbot\n",
    "2. **Understanding the Problem** - See why memory matters\n",
    "3. **Adding Short-Term Memory** - Make the chatbot remember conversations\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-openai langgraph\n",
    "```\n",
    "\n",
    "You'll also need an OpenAI API key set as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic Chatbot Without Memory\n",
    "\n",
    "Let's start with the simplest possible chatbot. This chatbot can respond to questions but **cannot remember previous messages**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from langchain.agents import create_agent\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv(dotenv_path=\"/home/bipin/Documents/genai/bot/learning/.env\")\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "\n",
    "# Set your API key (or use environment variable)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Simple Agent (No Memory)\n",
    "\n",
    "The `create_agent` function is the standard way to build agents in LangChain. It's built on LangGraph but provides a simpler interface for beginners.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `model`: The LLM to use (we'll use GPT-4)\n",
    "- `tools`: Functions the agent can use (empty for now)\n",
    "- `system_prompt`: Instructions for the chatbot's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple agent created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a basic agent without memory\n",
    "simple_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],  # No tools for now, just conversation\n",
    "    system_prompt=\"You are a helpful assistant. Answer questions concisely.\"\n",
    ")\n",
    "\n",
    "print(\"Simple agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test the Basic Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Nice to meet you, Bipin! How can I help today? If you’re not sure, tell me what you’d like to do—ask questions, get help with writing or coding, learn something new, or plan something.\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "response1 = simple_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bipin.\"}]\n",
    "})\n",
    "\n",
    "print(\"Bot:\", response1[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I don’t know your name. What would you like me to call you?\n"
     ]
    }
   ],
   "source": [
    "# Second message - asking about the name\n",
    "response2 = simple_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]\n",
    "})\n",
    "\n",
    "print(\"Bot:\", response2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Problem - No Memory!\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "You probably noticed the chatbot **doesn't remember your name**! This is because:\n",
    "\n",
    "1. Each `invoke()` call is independent\n",
    "2. We only sent the current message, not the conversation history\n",
    "3. The agent has no way to remember previous interactions\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "- Users expect chatbots to remember context\n",
    "- Real conversations build on previous messages\n",
    "- Without memory, the chatbot feels robotic and unhelpful\n",
    "\n",
    "### The Solution: Short-Term Memory\n",
    "\n",
    "We need to:\n",
    "1. Store conversation history\n",
    "2. Send the full history with each new message\n",
    "3. Use a \"checkpointer\" to persist the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Adding Short-Term Memory\n",
    "\n",
    "LangChain provides built-in memory management through **checkpointers**. A checkpointer saves the conversation state so the agent can remember previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Memory Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# MemorySaver stores conversation history in memory\n",
    "# For production, you'd use a database checkpointer (SQLite, Postgres, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an Agent with Memory\n",
    "\n",
    "The key difference: we add a `checkpointer` parameter to save conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent with memory created!\n"
     ]
    }
   ],
   "source": [
    "# Create a checkpointer to store conversation history\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create an agent WITH memory\n",
    "agent_with_memory = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a helpful assistant. Remember details from our conversation.\",\n",
    "    checkpointer=memory  # This enables memory!\n",
    ")\n",
    "\n",
    "print(\"Agent with memory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Understanding Thread IDs\n",
    "\n",
    "To maintain separate conversations, we use **thread IDs**. Think of a thread as a conversation session.\n",
    "\n",
    "- Each thread has its own memory\n",
    "- Same thread ID = same conversation\n",
    "- Different thread ID = new conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi Bipin! Nice to meet you. How can I help you today? I can assist with writing, explanations, coding, learning a topic, planning, or just chatting. Tell me what you’re aiming to do, and we’ll dive in.\n"
     ]
    }
   ],
   "source": [
    "# Configuration with thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "# First message in the conversation\n",
    "response1 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bipin.\"}]},\n",
    "    config  # Pass the config to maintain thread\n",
    ")\n",
    "\n",
    "print(\"Bot:\", response1[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Your name is Bipin. Nice to meet you again, Bipin! What would you like to work on today?\n"
     ]
    }
   ],
   "source": [
    "# Second message - the bot should remember the name now!\n",
    "response2 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config  # Same config = same conversation thread\n",
    ")\n",
    "\n",
    "print(\"Bot:\", response2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Nice to hear that, Bipin! Want to dive into one of these?\n",
      "\n",
      "- Pizza: recipe ideas, topping combos, crust types, or diet-friendly options\n",
      "- Cats: care tips, breed info, behavior tricks, or fun cat facts\n",
      "- A fun blend: a cat-themed pizza idea or a short story featuring a pizza-loving cat\n",
      "\n",
      "Tell me what you’d like to explore or if you have a specific question.\n"
     ]
    }
   ],
   "source": [
    "# Let's have a longer conversation to test memory\n",
    "response3 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"I like pizza and cats.\"}]},\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"Bot:\", response3[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: You like pizza and cats. Great combos!\n",
      "\n",
      "What would you like to do next? Here are a few options:\n",
      "- Pizza: recipe ideas, topping combos, or crust types\n",
      "- Cats: care tips, breed info, behavior tricks\n",
      "- A fun blend: a cat-themed pizza idea or a short story about a pizza-loving cat\n",
      "\n",
      "Tell me which you’d prefer or ask me anything specific.\n"
     ]
    }
   ],
   "source": [
    "# Ask about earlier information\n",
    "response4 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What do I like?\"}]},\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"Bot:\", response4[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bipin.', additional_kwargs={}, response_metadata={}, id='03a29872-7974-4577-8c30-1d565a46fecd'),\n",
       "  AIMessage(content='Hi Bipin! Nice to meet you. How can I help you today? I can assist with writing, explanations, coding, learning a topic, planning, or just chatting. Tell me what you’re aiming to do, and we’ll dive in.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 700, 'prompt_tokens': 30, 'total_tokens': 730, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 640, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CnwcKEyC7e3jBL9KcXdzQzeQgm8md', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2ef5-fcbc-7630-92e4-8f910ba4881f-0', usage_metadata={'input_tokens': 30, 'output_tokens': 700, 'total_tokens': 730, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 640}}),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='97ebaeaa-4c09-4e88-91d0-02630464a6ab'),\n",
       "  AIMessage(content='Your name is Bipin. Nice to meet you again, Bipin! What would you like to work on today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 353, 'prompt_tokens': 95, 'total_tokens': 448, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CnwcVBIBOcrqkYyBNzLGiT6fl6xLN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2ef6-24fb-7913-a41a-8eea41c6d6e6-0', usage_metadata={'input_tokens': 95, 'output_tokens': 353, 'total_tokens': 448, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}),\n",
       "  HumanMessage(content='I like pizza and cats.', additional_kwargs={}, response_metadata={}, id='75f228de-2c09-4376-8e23-17613e7bdebb'),\n",
       "  AIMessage(content='Nice to hear that, Bipin! Want to dive into one of these?\\n\\n- Pizza: recipe ideas, topping combos, crust types, or diet-friendly options\\n- Cats: care tips, breed info, behavior tricks, or fun cat facts\\n- A fun blend: a cat-themed pizza idea or a short story featuring a pizza-loving cat\\n\\nTell me what you’d like to explore or if you have a specific question.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 607, 'prompt_tokens': 135, 'total_tokens': 742, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CnwceRqDqr3XIjXWSeow6JqZAoETn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2ef6-4a48-7813-ae49-93760521a031-0', usage_metadata={'input_tokens': 135, 'output_tokens': 607, 'total_tokens': 742, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}}),\n",
       "  HumanMessage(content='What do I like?', additional_kwargs={}, response_metadata={}, id='11267590-b236-417e-a8c3-9936dd4819c0'),\n",
       "  AIMessage(content='You like pizza and cats. Great combos!\\n\\nWhat would you like to do next? Here are a few options:\\n- Pizza: recipe ideas, topping combos, or crust types\\n- Cats: care tips, breed info, behavior tricks\\n- A fun blend: a cat-themed pizza idea or a short story about a pizza-loving cat\\n\\nTell me which you’d prefer or ask me anything specific.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 345, 'prompt_tokens': 236, 'total_tokens': 581, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CnwcssBJ6zUKBmj7tThyCloB7Sju1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2ef6-84c6-7ff3-a7df-17c8d6e22203-0', usage_metadata={'input_tokens': 236, 'output_tokens': 345, 'total_tokens': 581, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Viewing Conversation History\n",
    "\n",
    "You can inspect what the agent remembers by looking at the messages in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Conversation History ===\n",
      "1. [Human]: Hi! My name is Bipin.\n",
      "2. [AI]: Hi Bipin! Nice to meet you. How can I help you today? I can assist with writing, explanations, coding, learning a topic, planning, or just chatting. Tell me what you’re aiming to do, and we’ll dive in.\n",
      "3. [Human]: What's my name?\n",
      "4. [AI]: Your name is Bipin. Nice to meet you again, Bipin! What would you like to work on today?\n",
      "5. [Human]: I like pizza and cats.\n",
      "6. [AI]: Nice to hear that, Bipin! Want to dive into one of these?\n",
      "\n",
      "- Pizza: recipe ideas, topping combos, crust types, or diet-friendly options\n",
      "- Cats: care tips, breed info, behavior tricks, or fun cat facts\n",
      "- A fun blend: a cat-themed pizza idea or a short story featuring a pizza-loving cat\n",
      "\n",
      "Tell me what you’d like to explore or if you have a specific question.\n",
      "7. [Human]: What do I like?\n",
      "8. [AI]: You like pizza and cats. Great combos!\n",
      "\n",
      "What would you like to do next? Here are a few options:\n",
      "- Pizza: recipe ideas, topping combos, or crust types\n",
      "- Cats: care tips, breed info, behavior tricks\n",
      "- A fun blend: a cat-themed pizza idea or a short story about a pizza-loving cat\n",
      "\n",
      "Tell me which you’d prefer or ask me anything specific.\n"
     ]
    }
   ],
   "source": [
    "# View all messages in the conversation\n",
    "print(\"\\n=== Full Conversation History ===\")\n",
    "for i, msg in enumerate(response4[\"messages\"], 1):\n",
    "    role = msg.__class__.__name__.replace(\"Message\", \"\")\n",
    "    content = msg.content\n",
    "    print(f\"{i}. [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Starting a New Conversation\n",
    "\n",
    "Use a different thread ID to start a fresh conversation with no shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New thread = new conversation = no memory of Alice\n",
    "new_config = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
    "\n",
    "response_new = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    new_config\n",
    ")\n",
    "\n",
    "print(\"Bot (new conversation):\", response_new[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building a Simple Interactive Chat Loop\n",
    "\n",
    "Let's create a simple chat interface where you can type messages and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot():\n",
    "    \"\"\"\n",
    "    Simple interactive chat loop.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Chat with your AI Assistant ===\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    # Create a new conversation thread\n",
    "    thread_config = {\"configurable\": {\"thread_id\": \"interactive_chat\"}}\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_message = input(\"You: \")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_message.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Send message to agent\n",
    "        response = agent_with_memory.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_message}]},\n",
    "            thread_config\n",
    "        )\n",
    "        \n",
    "        # Print bot response\n",
    "        print(f\"Bot: {response['messages'][-1].content}\\n\")\n",
    "\n",
    "# Uncomment to start chatting:\n",
    "# chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What You Learned\n",
    "\n",
    "### 1. Basic Chatbot (No Memory)\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    system_prompt=\"You are helpful.\"\n",
    ")\n",
    "```\n",
    "**Problem:** Forgets everything after each message.\n",
    "\n",
    "### 2. The Memory Problem\n",
    "- Without memory, chatbots can't maintain context\n",
    "- Each conversation starts from scratch\n",
    "- Poor user experience\n",
    "\n",
    "### 3. Chatbot with Short-Term Memory\n",
    "```python\n",
    "memory = MemorySaver()\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    system_prompt=\"You are helpful.\",\n",
    "    checkpointer=memory  # Enables memory!\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"chat_1\"}}\n",
    "agent.invoke({\"messages\": [...]}, config)\n",
    "```\n",
    "**Solution:** Remembers conversation within a thread.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **create_agent()** - The standard way to build agents in LangChain\n",
    "2. **Checkpointer** - Saves conversation state (MemorySaver, SQLite, Postgres)\n",
    "3. **Thread ID** - Identifies a conversation session\n",
    "4. **Short-term memory** - Remembers within a conversation thread\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Add tools** to let your agent perform actions\n",
    "- **Use persistent storage** (SQLite, Postgres) instead of in-memory\n",
    "- **Implement long-term memory** across multiple conversations\n",
    "- **Add message summarization** for very long conversations\n",
    "- **Stream responses** for real-time output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Understanding How Memory Works\n",
    "\n",
    "### Behind the Scenes\n",
    "\n",
    "When you use a checkpointer:\n",
    "\n",
    "1. **First message**: Agent processes it and saves the state\n",
    "2. **Second message**: Agent loads previous state, adds new message, processes, saves updated state\n",
    "3. **Continue**: This repeats for every message\n",
    "\n",
    "### Message Types\n",
    "\n",
    "- **HumanMessage**: Messages from the user\n",
    "- **AIMessage**: Messages from the assistant\n",
    "- **SystemMessage**: Instructions for the agent (system prompt)\n",
    "- **ToolMessage**: Results from tool executions\n",
    "\n",
    "### Memory Types in LangChain\n",
    "\n",
    "| Type | Scope | Use Case |\n",
    "|------|-------|----------|\n",
    "| **Short-term** | Within a thread | Single conversation session |\n",
    "| **Long-term** | Across threads | User preferences, facts across sessions |\n",
    "\n",
    "This tutorial covered **short-term memory**. For production chatbots, you'll often want both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Intelligent Cost Management - Trimming and Summarizing Messages\n",
    "\n",
    "### The Cost Problem\n",
    "\n",
    "As conversations grow longer (e.g., 20+ questions), the context window grows with every message. This leads to:\n",
    "\n",
    "- **Higher API costs**: You're charged for every token in the conversation history\n",
    "- **Slower responses**: More tokens = more processing time\n",
    "- **Context window limits**: Eventually, you'll hit the model's maximum token limit\n",
    "\n",
    "### The Solution: Two Intelligent Approaches\n",
    "\n",
    "1. **Message Trimming**: Keep only recent messages (fast but loses context)\n",
    "2. **Message Summarization**: Compress old messages into a summary (preserves context, slightly more cost)\n",
    "\n",
    "Let's learn both approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises for Part 5\n",
    "\n",
    "Try these to reinforce your learning:\n",
    "\n",
    "1. **Modify the trigger threshold**: Change the summarization trigger to 1000 tokens and observe the difference\n",
    "2. **Custom trimming logic**: Keep the first message, middle message, and last 2 messages\n",
    "3. **Cost comparison**: Run the same 20-question conversation with all three approaches and compare token counts\n",
    "4. **Hybrid approach**: Create middleware that trims after 10 messages AND summarizes at 1000 tokens\n",
    "5. **Smart summarization**: Write a custom summary prompt that focuses on extracting user preferences only\n",
    "\n",
    "### Bonus Challenge\n",
    "\n",
    "Create a chatbot that:\n",
    "- Uses trimming for the first 10 messages\n",
    "- Switches to summarization after 10 messages\n",
    "- Keeps a running summary of user preferences separately\n",
    "\n",
    "This combination gives you speed early on and context preservation later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5 Summary: Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **The Cost Problem**: Long conversations = growing token counts = higher costs\n",
    "2. **Message Trimming**: Keep only recent messages (simple, fast, loses old context)\n",
    "3. **Message Summarization**: Compress old messages into summaries (preserves context, slight overhead)\n",
    "4. **Token Counting**: How to measure and optimize token usage\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **For Customer Support**: Use summarization (need full conversation context)\n",
    "2. **For Quick Q&A**: Use trimming (only recent context matters)\n",
    "3. **For Production**: Combine both strategies with smart thresholds\n",
    "4. **Monitor Costs**: Track token usage to optimize your configuration\n",
    "\n",
    "### Code Reference\n",
    "\n",
    "```python\n",
    "# Trimming approach\n",
    "from langchain.agents.middleware import before_model\n",
    "\n",
    "@before_model\n",
    "def trim_middleware(state, runtime):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) <= 5:\n",
    "        return None\n",
    "    return {\"messages\": [messages[0]] + messages[-4:]}\n",
    "\n",
    "# Summarization approach\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "SummarizationMiddleware(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    trigger={\"tokens\": 2000},\n",
    "    keep={\"messages\": 8}\n",
    ")\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Experiment with different trigger thresholds\n",
    "- Try combining multiple middleware functions\n",
    "- Implement custom summarization prompts\n",
    "- Add persistent storage (SQLite, Postgres) for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Calculation Example\n",
    "\n",
    "Let's calculate potential savings with a 20-question conversation:\n",
    "\n",
    "### Without Memory Management\n",
    "- Message 1: 100 tokens sent\n",
    "- Message 2: 200 tokens sent (prev + new)\n",
    "- Message 3: 300 tokens sent\n",
    "- ...\n",
    "- Message 20: 2000 tokens sent\n",
    "- **Total tokens across all calls**: ~20,000 tokens\n",
    "\n",
    "### With Summarization (trigger at 500 tokens, keep 4 messages)\n",
    "- Messages 1-5: Normal growth (100 + 200 + 300 + 400 + 500 = 1,500 tokens)\n",
    "- Message 6+: Each call sends ~600 tokens (summary + last 4 messages)\n",
    "- **Total tokens across all calls**: ~10,000 tokens\n",
    "- **Savings**: ~50% reduction!\n",
    "\n",
    "### With Trimming (keep last 4 messages)\n",
    "- Every call sends only ~400 tokens (last 4 messages)\n",
    "- **Total tokens across all calls**: ~8,000 tokens\n",
    "- **Savings**: ~60% reduction!\n",
    "\n",
    "**Note**: These are simplified calculations. Actual savings depend on message lengths and conversation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready chatbot with intelligent memory management\n",
    "production_memory = MemorySaver()\n",
    "\n",
    "production_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",  # Cheap model for summarization\n",
    "            trigger=[\n",
    "                {\"tokens\": 2000},    # Trigger at 2000 tokens OR\n",
    "                {\"messages\": 15}     # Trigger at 15 messages\n",
    "            ],\n",
    "            keep={\"messages\": 8}    # Keep last 8 messages (4 conversation turns)\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=production_memory\n",
    ")\n",
    "\n",
    "print(\"Production-ready chatbot created!\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"- Summarizes when: >2000 tokens OR >15 messages\")\n",
    "print(\"- Keeps intact: Last 8 messages\")\n",
    "print(\"- Summarization model: gpt-4o-mini (cost-effective)\")\n",
    "print(\"- Main model: gpt-4o (high quality responses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Example: Production-Ready Configuration\n",
    "\n",
    "Here's a recommended setup for a production chatbot that balances cost and context retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import token counting utilities\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "def analyze_token_usage(response):\n",
    "    \"\"\"\n",
    "    Analyze token usage in a conversation.\n",
    "    \"\"\"\n",
    "    messages = response[\"messages\"]\n",
    "    \n",
    "    # Count tokens in all messages\n",
    "    total_tokens = count_tokens_approximately(messages)\n",
    "    \n",
    "    print(f\"Total messages: {len(messages)}\")\n",
    "    print(f\"Approximate tokens: {total_tokens}\")\n",
    "    \n",
    "    # Show token breakdown by message type\n",
    "    human_msgs = [m for m in messages if m.__class__.__name__ == \"HumanMessage\"]\n",
    "    ai_msgs = [m for m in messages if m.__class__.__name__ == \"AIMessage\"]\n",
    "    \n",
    "    print(f\"Human messages: {len(human_msgs)}\")\n",
    "    print(f\"AI messages: {len(ai_msgs)}\")\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "# Test with our previous responses\n",
    "print(\"=== Without Management (Full History) ===\")\n",
    "analyze_token_usage(response4)\n",
    "\n",
    "print(\"\\n=== With Trimming ===\")\n",
    "# You would analyze the trimming response here\n",
    "\n",
    "print(\"\\n=== With Summarization ===\")\n",
    "# You would analyze the summary response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced: Token Counting\n",
    "\n",
    "Let's see exactly how many tokens we're using with each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing the Three Approaches\n",
    "\n",
    "| Approach | Memory | Cost Efficiency | Context Retention | Best For |\n",
    "|----------|--------|----------------|-------------------|----------|\n",
    "| **No Management** | Unlimited growth | Poor (grows linearly) | Perfect | Short conversations |\n",
    "| **Trimming** | Recent messages only | Excellent | Limited | Cost-sensitive, recent context matters |\n",
    "| **Summarization** | Summary + recent | Good | Excellent | Long conversations, full context needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Summarization Works\n",
    "\n",
    "When the token count exceeds the threshold (500 tokens):\n",
    "\n",
    "1. The middleware takes old messages (except the last 4)\n",
    "2. Sends them to a cheaper model (gpt-4o-mini) with a prompt: \"Summarize this conversation\"\n",
    "3. Replaces the old messages with a single summary message\n",
    "4. Keeps recent messages intact for immediate context\n",
    "\n",
    "**Result:** The agent remembers details from the beginning of the conversation (via summary) AND recent context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization with a long conversation\n",
    "summary_config = {\"configurable\": {\"thread_id\": \"summary_test\"}}\n",
    "\n",
    "# Same messages as before\n",
    "messages_to_send = [\n",
    "    \"Hi! My name is Bipin.\",\n",
    "    \"I live in Mumbai.\",\n",
    "    \"I work as a software engineer.\",\n",
    "    \"I love playing cricket.\",\n",
    "    \"My favorite food is biryani.\",\n",
    "    \"I have a dog named Max.\",\n",
    "    \"I also enjoy reading science fiction books.\",\n",
    "    \"My favorite author is Isaac Asimov.\",\n",
    "    \"What's my name?\",\n",
    "    \"What do you know about me?\",  # This should include summarized info!\n",
    "]\n",
    "\n",
    "print(\"Sending messages to agent with summarization...\\n\")\n",
    "\n",
    "for i, user_msg in enumerate(messages_to_send, 1):\n",
    "    response = agent_with_summarization.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_msg}]},\n",
    "        summary_config\n",
    "    )\n",
    "    bot_response = response[\"messages\"][-1].content\n",
    "    \n",
    "    print(f\"{i}. You: {user_msg}\")\n",
    "    print(f\"   Bot: {bot_response[:150]}...\")  # Truncate for readability\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Summarization\n",
    "\n",
    "Let's test with the same long conversation and see how the agent preserves context through summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the SummarizationMiddleware Parameters\n",
    "\n",
    "- `model`: The LLM to use for creating summaries (use a cheaper model like gpt-4o-mini)\n",
    "- `trigger`: When to start summarization\n",
    "  - `{\"tokens\": 500}` - Trigger when conversation exceeds 500 tokens\n",
    "  - `{\"messages\": 10}` - Trigger when there are 10+ messages\n",
    "  - Can use both: `[{\"tokens\": 500}, {\"messages\": 10}]` (OR logic)\n",
    "- `keep`: How many recent messages to keep intact\n",
    "  - `{\"messages\": 4}` - Keep last 4 messages\n",
    "  - `{\"tokens\": 200}` - Keep last 200 tokens worth of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import summarization middleware\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "# Create an agent with automatic summarization\n",
    "summary_memory = MemorySaver()\n",
    "\n",
    "agent_with_summarization = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",  # Use a cheaper model for summarization\n",
    "            trigger={\"tokens\": 500},  # Trigger summarization at 500 tokens\n",
    "            keep={\"messages\": 4}  # Keep the last 4 messages intact\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=summary_memory\n",
    ")\n",
    "\n",
    "print(\"Agent with summarization middleware created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Approach 2: Message Summarization (Intelligent Compression)\n",
    "\n",
    "Instead of deleting old messages, we can **summarize** them! This preserves important context while reducing token count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations with Trimming\n",
    "\n",
    "- The agent remembers your name (recent messages)\n",
    "- But it might forget earlier details like where you work (trimmed messages)\n",
    "- The message count stays bounded (doesn't grow indefinitely)\n",
    "- **Cost savings**: Only recent messages are sent to the LLM each time\n",
    "\n",
    "### Pros and Cons of Trimming\n",
    "\n",
    "**Pros:**\n",
    "- Fast and simple\n",
    "- Guaranteed cost savings\n",
    "- No extra LLM calls needed\n",
    "\n",
    "**Cons:**\n",
    "- Loses information from trimmed messages\n",
    "- May forget important context from earlier in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a long conversation\n",
    "trim_config = {\"configurable\": {\"thread_id\": \"trim_test\"}}\n",
    "\n",
    "# Send multiple messages\n",
    "messages_to_send = [\n",
    "    \"Hi! My name is Bipin.\",\n",
    "    \"I live in Mumbai.\",\n",
    "    \"I work as a software engineer.\",\n",
    "    \"I love playing cricket.\",\n",
    "    \"My favorite food is biryani.\",\n",
    "    \"I have a dog named Max.\",\n",
    "    \"What's my name?\",  # This should still work\n",
    "    \"Where do I work?\",  # This might be forgotten due to trimming\n",
    "]\n",
    "\n",
    "print(\"Sending messages to agent with trimming...\\n\")\n",
    "\n",
    "for i, user_msg in enumerate(messages_to_send, 1):\n",
    "    response = agent_with_trimming.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_msg}]},\n",
    "        trim_config\n",
    "    )\n",
    "    bot_response = response[\"messages\"][-1].content\n",
    "    \n",
    "    print(f\"{i}. You: {user_msg}\")\n",
    "    print(f\"   Bot: {bot_response}\")\n",
    "    print(f\"   Messages in history: {len(response['messages'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Message Trimming\n",
    "\n",
    "Let's simulate a long conversation with 10+ messages and see how trimming works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with message trimming middleware\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "trim_memory = MemorySaver()\n",
    "\n",
    "agent_with_trimming = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],\n",
    "    middleware=[trim_messages_middleware],  # Add our trimming middleware\n",
    "    checkpointer=trim_memory\n",
    ")\n",
    "\n",
    "print(\"Agent with message trimming created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for message trimming\n",
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "# Define a middleware function that trims messages\n",
    "@before_model\n",
    "def trim_messages_middleware(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    Keep only the last few messages to fit context window.\n",
    "    This runs BEFORE each model call.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If we have 5 or fewer messages, no need to trim\n",
    "    if len(messages) <= 5:\n",
    "        return None  # No changes needed\n",
    "    \n",
    "    # Keep the first message (usually system/initial context)\n",
    "    first_msg = messages[0]\n",
    "    \n",
    "    # Keep the last 4 messages (2 turns of conversation)\n",
    "    recent_messages = messages[-4:]\n",
    "    \n",
    "    # Create new message list\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "    \n",
    "    # Return the trimmed messages\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),  # Remove all existing messages\n",
    "            *new_messages  # Add back only the messages we want to keep\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"Trim middleware function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Approach 1: Message Trimming with Middleware\n",
    "\n",
    "The simplest approach: automatically keep only the last N messages. LangChain provides middleware that runs **before** the model is called."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
